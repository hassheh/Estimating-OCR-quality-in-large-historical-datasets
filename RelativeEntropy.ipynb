{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv made some mistakes when reading the tsv files\n",
    "# this function is able to read the files more reliably\n",
    "def read_csv(path, sep= ',', names=[\"PAGENAME\", \"OCR\", \"GT\"]):\n",
    "    with open(path, 'r', encoding='utf8') as file:\n",
    "        rows = file.read().split(\"\\n\")\n",
    "        data = np.array([row.split(sep) for row in rows if len(row.split(sep)) == 3])\n",
    "    return pd.DataFrame(data=data, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_file_0 = \"../ocr_data/nlf_ocr_gt_tescomb5_2017.xlsx\"\n",
    "path = '../ocr_data/'\n",
    "folders = [f for f in glob.glob(path + \"**/*.tsv\", recursive=True)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = pd.concat([read_csv(path, sep = \"\\t\", names=[\"PAGENAME\", \"OCR\", \"GT\"]) for path in folders])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the excel file takes a while\n",
    "df_full = pd.read_excel(ocr_file_0, sheet_name=\"Words\", header=3, skip_rows=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_full.values[:,5].astype(np.str)\n",
    "ocr_tess = df_full.values[:,6].astype(np.str)\n",
    "ocr_old = df_full.values[:,7].astype(np.str)\n",
    "ocr_fr11 = df_full.values[:,8].astype(np.str)\n",
    "ocr_sampled = df_sampled.values[:,1].astype(np.str)\n",
    "ocr_sampled_gt = df_sampled.values[:,2].astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'että kunnille sallittaisiin walta sulkea kapakat alueellaan, mutta wielä ei ole semmoinen laki tullut toimeen. Parlamentti on tosin kerran h'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the individual words into pages\n",
    "def join(array):\n",
    "    return \" \".join(map(str, array))\n",
    "def combine_to_pages(dataframe, column):\n",
    "    pages = np.split(dataframe.values[:, column], np.cumsum(np.unique(dataframe.values[:, 3], return_counts=True)[1])[:-1])\n",
    "    return np.array(list(map(join, pages)))\n",
    "pages_gt = combine_to_pages(df_full, 5)\n",
    "pages_tess = combine_to_pages(df_full, 6)\n",
    "pages_old = combine_to_pages(df_full, 7)\n",
    "pages_fr11 = combine_to_pages(df_full, 8)\n",
    "ocr_sampled = df_sampled.values[:,1]\n",
    "pages_gt[1][:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(string):\n",
    "        \"Calculates the Shannon entropy of a string\"\n",
    "        string = str(string)\n",
    "\n",
    "        # get probability of chars in string\n",
    "        prob = [ float(string.count(c)) / len(string) for c in dict.fromkeys(list(string)) ]\n",
    "\n",
    "        # calculate the entropy\n",
    "        entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])\n",
    "\n",
    "        return entropy\n",
    "    \n",
    "def per_char_entropy(string):\n",
    "    length = max(len(str(string)),1)\n",
    "    return entropy(string) / length\n",
    "\n",
    "\n",
    "def entropy_ideal(length):\n",
    "        \"Calculates the ideal Shannon entropy of a string with given length\"\n",
    "\n",
    "        prob = 1.0 / length\n",
    "\n",
    "        return -1.0 * length * prob * math.log(prob) / math.log(2.0)\n",
    "vectorEntropy = np.vectorize(entropy)\n",
    "vectorPerCharEntropy = np.vectorize(per_char_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ü', 5), ('Å', 5), ('#', 4), ('™', 1), ('Ü', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_tess = collections.Counter(\"\".join(pages_tess))\n",
    "counts_old = collections.Counter(\"\".join(pages_old))\n",
    "counts_fr11 = collections.Counter(\"\".join(pages_fr11))\n",
    "counts_gt = collections.Counter(\"\".join(pages_gt))\n",
    "counts_sampled = collections.Counter(\"\".join(ocr_sampled))\n",
    "len_tess = len(\"\".join(pages_tess))\n",
    "len_old = len(\"\".join(pages_old))\n",
    "len_fr11 = len(\"\".join(pages_fr11))\n",
    "len_gt = len(\"\".join(pages_gt))\n",
    "len_sampled = len(\"\".join(ocr_sampled))\n",
    "list(sorted(counts_old.items(), key = lambda x: x[1], reverse=True))[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.419403082960818"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def KL_divergence(string_ratio_corpus_ratio):\n",
    "    string_ratio, corpus_ratio = string_ratio_corpus_ratio\n",
    "    return string_ratio*np.log2(string_ratio / corpus_ratio)\n",
    "    \n",
    "def mean_KL_divergence(string, counts_data, len_data):\n",
    "    string = str(string)\n",
    "    if(len(string)==0):\n",
    "        return 0\n",
    "    string_ratio = map(lambda char: string.count(char) / len(string), string)\n",
    "    corpus_ratio = map(lambda char: counts_data[char] / len_data, string)\n",
    "    KL_div = list(map(KL_divergence, zip(string_ratio, corpus_ratio)))\n",
    "    return np.mean(KL_div) #max and mean have similar results, sum finds longer words\n",
    "    \n",
    "    \n",
    "mean_KL_divergence(\"üö\", counts_old, len_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tess_KL(string):\n",
    "    return mean_KL_divergence(string, counts_tess, len_tess)\n",
    "def map_old_KL(string):\n",
    "    return mean_KL_divergence(string, counts_old, len_old)\n",
    "def map_fr11_KL(string):\n",
    "    return mean_KL_divergence(string, counts_fr11, len_fr11)\n",
    "def map_gt_KL(string):\n",
    "    return mean_KL_divergence(string, counts_gt, len_gt)\n",
    "def map_sampled_KL(string):\n",
    "    return mean_KL_divergence(string, counts_sampled, len_sampled)\n",
    "kl_tess = np.array(list(map(map_tess_KL, ocr_tess)))\n",
    "kl_old = np.array(list(map(map_old_KL, ocr_old)))\n",
    "kl_fr11 = np.array(list(map(map_fr11_KL, ocr_fr11)))\n",
    "kl_gt = np.array(list(map(map_gt_KL, ground_truth)))\n",
    "kl_sampled = np.array(list(map(map_sampled_KL, ocr_sampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of estimation tess 0.18650327580696713\n",
      "error of estimation old 0.29585930009587724\n",
      "error of estimation fr11 0.23069071588366885\n",
      "predicted_quality tess 0.9999240971556408\n",
      "predicted_quality old 0.999938079258549\n",
      "predicted_quality fr11 0.9999960051134548\n",
      "predicted_quality gt 0.9996524448705657\n",
      "predicted_quality sampled 1.0\n",
      "true_quality tess 0.8135246883988495\n",
      "true_quality old 0.7041506871204858\n",
      "true_quality fr11 0.7693092841163311\n"
     ]
    }
   ],
   "source": [
    "kl_treshold = 16\n",
    "kl_treshold_tess_int = (kl_tess<kl_treshold)*1\n",
    "kl_treshold_old_int = (kl_old<kl_treshold)*1\n",
    "kl_treshold_fr11_int = (kl_fr11<kl_treshold)*1\n",
    "kl_treshold_gt_int = (kl_gt<kl_treshold)*1\n",
    "kl_treshold_sampled_int = (kl_sampled<kl_treshold)*1\n",
    "\n",
    "print(\"error of estimation tess\", 1-np.mean(kl_treshold_tess_int*1 == df_full.values[:,9]))\n",
    "print(\"error of estimation old\", 1-np.mean(kl_treshold_old_int*1 == df_full.values[:,10]))\n",
    "print(\"error of estimation fr11\", 1-np.mean(kl_treshold_fr11_int*1 == df_full.values[:,11]))\n",
    "print(\"predicted_quality tess\", np.mean(kl_treshold_tess_int))\n",
    "print(\"predicted_quality old\", np.mean(kl_treshold_old_int))\n",
    "print(\"predicted_quality fr11\", np.mean(kl_treshold_fr11_int))\n",
    "print(\"predicted_quality gt\", np.mean(kl_treshold_gt_int))\n",
    "print(\"predicted_quality sampled\", np.mean(kl_treshold_sampled_int))\n",
    "print(\"true_quality tess\", np.mean(df_full.values[:,9]))\n",
    "print(\"true_quality old\", np.mean(df_full.values[:,10]))\n",
    "print(\"true_quality fr11\", np.mean(df_full.values[:,11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['®' 'Q' '+' '+' '\\\\\\\\' '*' 'Z' '+' '+' '+' '+' '+' '+' '+' '+' '%' 'X'\n",
      " 'Q' 'Ö' 'Q' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+'\n",
      " '+' 'Q' '+']\n",
      "['O' 'Helsingissä,' '+' '+' '\"' '½' 'Zefanias' '+' '+' '+' '+' '+' '+' '+'\n",
      " '+' '%' 'X' 'Korhonen' 'Toiwotan' 'Awaimia' '+' '+' '+' '4' '+' '+' '+'\n",
      " '+' '+' '+' '+' '+' '\"' '+' '+' '+' 'L.' '+9']\n",
      "['&' '&' '&' '&' '%' '&' '&' '&' '{' '•' '•' '&' '&' '+' '%' '%' '&' '%'\n",
      " '&' '_' '+' '__' '+' '+' '+' 'É' '•' '•' '&' '%' '&']\n",
      "['&' '&' '&' '&' '%,' 'à' '&' '&' '½' '\"' '\"' '&' '&' '+' '%' 'laina' 'L'\n",
      " '%' '&' '—' '+' '—' '+' '+' '+' '&' '3.738:' 'A.' 'Lignell' '%' '&']\n"
     ]
    }
   ],
   "source": [
    "# a lot of the tokens marked wrong are actually correct, just unusual\n",
    "marked_wrong_tess = [i for i, x in enumerate(kl_tess) if x>kl_treshold]\n",
    "print(ocr_tess[marked_wrong_tess])\n",
    "print(ground_truth[marked_wrong_tess])\n",
    "marked_wrong_old = [i for i, x in enumerate(kl_old) if x>kl_treshold]\n",
    "print(ocr_old[marked_wrong_old])\n",
    "print(ground_truth[marked_wrong_old])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352508 148101 13 18\n"
     ]
    }
   ],
   "source": [
    "# True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "TP = np.sum(np.logical_and(kl_treshold_old_int == 1, df_full.values[:,10] == 1))\n",
    "\n",
    "# True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "TN = np.sum(np.logical_and(kl_treshold_old_int == 0, df_full.values[:,10] == 0))\n",
    " \n",
    "# False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "FP = np.sum(np.logical_and(kl_treshold_old_int == 1, df_full.values[:,10] == 0))\n",
    " \n",
    "# False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "FN = np.sum(np.logical_and(kl_treshold_old_int == 0, df_full.values[:,10] == 1))\n",
    "\n",
    "print(TP,FP,TN,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
